---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')

titles = []
dates = []
links = []
categories = []

enforcement_items = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')

for item in enforcement_items:
    title_tag = item.find('h2', class_='usa-card__heading')
    title = title_tag.get_text(strip=True)
    titles.append(title)

    date_tag = item.find('span', class_='text-base-dark padding-right-105')
    date = date_tag.get_text(strip=True) 
    dates.append(date)

    category_tag = item.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_tag.get_text(strip=True)
    categories.append(category)

    link_tag = title_tag.find('a', href=True) 
    link = link_tag['href'] if link_tag else 'N/A'
    if not link.startswith('http'):
        full_link = f'https://oig.hhs.gov{link}'
    else:
        full_link = link
    links.append(full_link)

df = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links
})

print(df.head())

```

  
### 2. Crawling (PARTNER 1)

```{python}
import time

agencies = []

for full_link in links:
    action_response = requests.get(full_link)
    action_response.raise_for_status()
    action_soup = BeautifulSoup(action_response.text, 'html.parser')

    agency_tag = action_soup.find('span', string='Agency:')
    if agency_tag:
        agency = agency_tag.find_parent('li').get_text(
            strip=True).replace('Agency:', '').strip()
    else:
        agency = 'N/A'

    agencies.append(agency)

    time.sleep(1)

df = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links,
    'Agency': agencies
})

print(df.head())


```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

```{python}
Function scrape_enforcement_actions(start_month, start_year):
    # 1. Input Validation
    If start_year < 2013:
        Print "Year must be >= 2013"
        Return

    # 2. Initialization
    Initialize an empty DataFrame: enforcement_data
    Set the starting date to (start_year, start_month)
    Set the current date to today's month and year
    Generate the initial URL

    # 3. Page Scraping Loop (From start date to current date)
    current_page = 1 
    continue_scraping = True  

    While continue_scraping is True:
        # 4. Construct URL for the Main Page
        If current_page == 1:
            Construct the first page URL (e.g., "https://oig.hhs.gov/fraud/enforcement/")
        Else:
            Construct paginated URL (e.g., "https://oig.hhs.gov/fraud/enforcement/?page=current_page")

        # 5. Fetch Page Content
        Use requests library to get the page content
        Use BeautifulSoup to parse the page content

        # 6. Extract Enforcement Actions
        Find all enforcement action items on the page
        For each enforcement action item:
            Extract title, date, category, and link
            Append the data to a temporary DataFrame

            # 7. Deep Scraping of Detailed Information
            Use the link to visit the detailed page
            Extract the agency name from the page
            Add all the data to the main DataFrame

        # 8. Check for Next Page
        If "Next Page" button is found:
            current_page += 1  
        Else:
            continue_scraping = False 

        # 9. Add Delay Between Requests
        Wait for 1 second

    # 10. Save Data to CSV
    filename = "enforcement_actions_<start_year>_<start_month>.csv"
    Save the DataFrame as a CSV file

```

* b. Create Dynamic Scraper (PARTNER 2)

```{python}

def scrape_enforcement_actions(start_month, start_year):
    if start_year < 2013:
        print("Year must be >= 2013")
        return

    enforcement_data = pd.DataFrame(columns=['Title', 'Date', 'Category', 'Link', 'Agency'])
    current_date = datetime.now()
    current_page = 1
    continue_scraping = True

    while continue_scraping:
        if current_page == 1:
            url = "https://oig.hhs.gov/fraud/enforcement/"
        else:
            url = f"https://oig.hhs.gov/fraud/enforcement/?page={current_page}"

        response = requests.get(url)
        if response.status_code != 200:
            print(f"Failed to retrieve page {current_page}")
            break
        soup = BeautifulSoup(response.text, 'html.parser')

        enforcement_items = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
        if not enforcement_items:
            continue_scraping = False
            break

        for item in enforcement_items:
            title_tag = item.find('h2', class_='usa-card__heading')
            title = title_tag.get_text(strip=True) if title_tag else 'N/A'

            date_tag = item.find('span', class_='text-base-dark padding-right-105')
            date = date_tag.get_text(strip=True) if date_tag else 'N/A'

            category_tag = item.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
            category = category_tag.get_text(strip=True) if category_tag else 'N/A'

            link_tag = title_tag.find('a', href=True) if title_tag else None
            link = link_tag['href'] if link_tag else 'N/A'
            if link != 'N/A' and not link.startswith('http'):
                link = f'https://oig.hhs.gov{link}'

            if link != 'N/A':
                action_response = requests.get(link)
                if action_response.status_code == 200:
                    action_soup = BeautifulSoup(action_response.text, 'html.parser')
                    agency_tag = action_soup.find('span', string='Agency:')
                    if agency_tag:
                        agency = agency_tag.find_parent('li').get_text(strip=True).replace('Agency:', '').strip()
                    else:
                        agency = 'N/A'
                else:
                    agency = 'N/A'
            else:
                agency = 'N/A'

            enforcement_data = pd.concat([enforcement_data, pd.DataFrame([{
                'Title': title,
                'Date': date,
                'Category': category,
                'Link': link,
                'Agency': agency
            }])], ignore_index=True)

        next_page_tag = soup.find('a', class_='next-page-link')
        if next_page_tag:
            current_page += 1
        else:
            continue_scraping = False

        time.sleep(1)

    filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    enforcement_data.to_csv(filename, index=False)
    
    return enforcement_data

df = scrape_enforcement_actions(1, 2023)

print(df.head())

total_actions = len(df)
early_action = df.iloc[df['Date'].idxmin()]
print(f"Total enforcement actions collected: {total_actions}")
print(f"Earliest enforcement action: {early_action}")

```

* c. Test Partner's Code (PARTNER 1)

```{python}

def scrape_enforcement_actions(start_month, start_year):
    if start_year < 2013:
        print("Year must be >= 2013")
        return

    enforcement_data = pd.DataFrame(columns=['Title', 'Date', 'Category', 'Link', 'Agency'])
    current_date = datetime.now()
    current_page = 1
    continue_scraping = True

    while continue_scraping:
        if current_page == 1:
            url = "https://oig.hhs.gov/fraud/enforcement/"
        else:
            url = f"https://oig.hhs.gov/fraud/enforcement/?page={current_page}"

        response = requests.get(url)
        if response.status_code != 200:
            print(f"Failed to retrieve page {current_page}")
            break
        soup = BeautifulSoup(response.text, 'html.parser')

        enforcement_items = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
        if not enforcement_items:
            continue_scraping = False
            break

        for item in enforcement_items:
            title_tag = item.find('h2', class_='usa-card__heading')
            title = title_tag.get_text(strip=True) if title_tag else 'N/A'

            date_tag = item.find('span', class_='text-base-dark padding-right-105')
            date = date_tag.get_text(strip=True) if date_tag else 'N/A'

            category_tag = item.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
            category = category_tag.get_text(strip=True) if category_tag else 'N/A'

            link_tag = title_tag.find('a', href=True) if title_tag else None
            link = link_tag['href'] if link_tag else 'N/A'
            if link != 'N/A' and not link.startswith('http'):
                link = f'https://oig.hhs.gov{link}'

            if link != 'N/A':
                action_response = requests.get(link)
                if action_response.status_code == 200:
                    action_soup = BeautifulSoup(action_response.text, 'html.parser')
                    agency_tag = action_soup.find('span', string='Agency:')
                    if agency_tag:
                        agency = agency_tag.find_parent('li').get_text(strip=True).replace('Agency:', '').strip()
                    else:
                        agency = 'N/A'
                else:
                    agency = 'N/A'
            else:
                agency = 'N/A'

            enforcement_data = pd.concat([enforcement_data, pd.DataFrame([{
                'Title': title,
                'Date': date,
                'Category': category,
                'Link': link,
                'Agency': agency
            }])], ignore_index=True)

        next_page_tag = soup.find('a', class_='next-page-link')
        if next_page_tag:
            current_page += 1
        else:
            continue_scraping = False

        time.sleep(1)

    filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    enforcement_data.to_csv(filename, index=False)
    
    return enforcement_data

df = scrape_enforcement_actions(1, 2021)

print(df.head())

total_actions = len(df)
early_action = df.iloc[df['Date'].idxmin()]
print(f"Total enforcement actions collected: {total_actions}")
print(f"Earliest enforcement action: {early_action}")
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```